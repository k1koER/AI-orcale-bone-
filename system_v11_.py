# -*- coding: utf-8 -*-
"""system V11 .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fTnHF14zY79OLC4x59eotS9J1N5tADSJ
"""

import subprocess
import os
import time

print("ğŸ“¦ ä¿®å¤ NumPy...")
subprocess.run(['pip', 'install', 'numpy==1.26.4', '-q'], capture_output=True)

from google.colab import drive
drive.mount('/content/drive')

print("ğŸ“¦ å®‰è£…ä¾èµ–...")
subprocess.run(['pip', 'install', 'f5-tts', 'flask', 'flask-cors', 'pyngrok',
                'soundfile', 'librosa', 'moviepy', 'pillow', 'gtts', 'ollama',
                'diffusers', 'transformers', 'accelerate', 'safetensors', '-q'], capture_output=True)

print("ğŸ“¦ å®‰è£… Ollama...")
subprocess.run(['curl', '-fsSL', 'https://ollama.com/install.sh', '-o', '/tmp/install.sh'], capture_output=True)
subprocess.run(['sh', '/tmp/install.sh'], capture_output=True, text=True)

subprocess.Popen(['ollama', 'serve'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
time.sleep(5)

print("ğŸ“¥ ä¸‹è½½ Gemma 3 4B...")
subprocess.run(['ollama', 'pull', 'gemma3:4b'])

print("ğŸ”§ è®¾ç½® ngrok...")
subprocess.run(['ngrok', 'config', 'add-authtoken', '35yagLESS5O0a1grPmIHdEJnp5S_4pi3TBc9b5bSpd5kp8SEM'], capture_output=True)

print("âœ… Cell 1 å®Œæˆï¼è¯·é‡å¯ Runtime åæ‰§è¡Œ Cell 2")

# ==========================================
# Cell 2: éª¨æ„Ÿå­—å…¸åç«¯ v11-fixed2
# ä¿®æ­£ï¼šOllama timeoutã€æ®µè½ç”Ÿæˆé‚è¼¯
# ==========================================

import os
import sys
import re
import json
import base64
import io
import torch
import numpy as np
import soundfile as sf
from PIL import Image, ImageDraw, ImageFont
from flask import Flask, request, jsonify, Response, stream_with_context
from flask_cors import CORS
import ollama
from gtts import gTTS
import traceback
import gc
import librosa
import subprocess
import time
import threading

from f5_tts.api import F5TTS

app = Flask(__name__)
CORS(app)

OLLAMA_MODEL = "gemma3:4b"
VISION_MODEL = "gemma3:4b"

f5_model = None
sdxl_pipe = None
svd_pipe = None

F5_NFE_STEP = 64
F5_CFG_STRENGTH = 2.0
F5_SPEED = 0.9

SVD_MOTION = 80
SVD_FRAMES = 25
SVD_FPS = 6

VOICE_REFERENCES = {
    "villager_zh": {"path": "/content/drive/MyDrive/Voice data/Minecraft Villager chinese.wav", "text": "å“‡ï¼ä½ çœ‹è¿™ä¸ªç”²éª¨æ–‡å­—ï¼å®ƒå°±åƒä¸€ä¸ªå°å›¾ç”»ä¸€æ ·ï¼Œå¤ªç¥å¥‡äº†ï¼è¿™å°±æ˜¯ä¸‰åƒå¹´å‰ç¥–å…ˆçš„æ™ºæ…§å•Šï¼"},
    "trump_zh": {"path": "/content/drive/MyDrive/Voice data/Donald Trump chinese.wav", "text": "å“‡ï¼ä½ çœ‹è¿™ä¸ªç”²éª¨æ–‡å­—ï¼å®ƒå°±åƒä¸€ä¸ªå°å›¾ç”»ä¸€æ ·ï¼Œå¤ªç¥å¥‡äº†ï¼è¿™å°±æ˜¯ä¸‰åƒå¹´å‰ç¥–å…ˆçš„æ™ºæ…§å•Šï¼"},
    "obama_zh": {"path": "/content/drive/MyDrive/Voice data/Barack Obama chinese.wav", "text": "å“‡ï¼ä½ çœ‹è¿™ä¸ªç”²éª¨æ–‡å­—ï¼å®ƒå°±åƒä¸€ä¸ªå°å›¾ç”»ä¸€æ ·ï¼Œå¤ªç¥å¥‡äº†ï¼è¿™å°±æ˜¯ä¸‰åƒå¹´å‰ç¥–å…ˆçš„æ™ºæ…§å•Šï¼"},
    "biden_zh": {"path": "/content/drive/MyDrive/Voice data/Joe Biden chinese.wav", "text": "å“‡ï¼ä½ çœ‹è¿™ä¸ªç”²éª¨æ–‡å­—ï¼å®ƒå°±åƒä¸€ä¸ªå°å›¾ç”»ä¸€æ ·ï¼Œå¤ªç¥å¥‡äº†ï¼è¿™å°±æ˜¯ä¸‰åƒå¹´å‰ç¥–å…ˆçš„æ™ºæ…§å•Šï¼"},
    "taylor_zh": {"path": "/content/drive/MyDrive/Voice data/Taylor Swift chinese.wav", "text": "å“‡ï¼ä½ çœ‹è¿™ä¸ªç”²éª¨æ–‡å­—ï¼å®ƒå°±åƒä¸€ä¸ªå°å›¾ç”»ä¸€æ ·ï¼Œå¤ªç¥å¥‡äº†ï¼è¿™å°±æ˜¯ä¸‰åƒå¹´å‰ç¥–å…ˆçš„æ™ºæ…§å•Šï¼"},
    "morgan_zh": {"path": "/content/drive/MyDrive/Voice data/Morgan Freeman chinese.wav", "text": "å“‡ï¼ä½ çœ‹è¿™ä¸ªç”²éª¨æ–‡å­—ï¼å®ƒå°±åƒä¸€ä¸ªå°å›¾ç”»ä¸€æ ·ï¼Œå¤ªç¥å¥‡äº†ï¼è¿™å°±æ˜¯ä¸‰åƒå¹´å‰ç¥–å…ˆçš„æ™ºæ…§å•Šï¼"},
    "hillary_zh": {"path": "/content/drive/MyDrive/Voice data/Hillary Clinton chinese.wav", "text": "å“‡ï¼ä½ çœ‹è¿™ä¸ªç”²éª¨æ–‡å­—ï¼å®ƒå°±åƒä¸€ä¸ªå°å›¾ç”»ä¸€æ ·ï¼Œå¤ªç¥å¥‡äº†ï¼è¿™å°±æ˜¯ä¸‰åƒå¹´å‰ç¥–å…ˆçš„æ™ºæ…§å•Šï¼"},
    "villager_en": {"path": "/content/drive/MyDrive/Voice data/Minecraft Villager english.wav", "text": "Wow! Look at this oracle bone character! It's like a tiny picture! So amazing! This is the wisdom of our ancestors from three thousand years ago!"},
    "trump_en": {"path": "/content/drive/MyDrive/Voice data/Donald Trump english.wav", "text": "Wow! Look at this oracle bone character! It's like a tiny picture! So amazing! This is the wisdom of our ancestors from three thousand years ago!"},
    "obama_en": {"path": "/content/drive/MyDrive/Voice data/Barack Obama english.wav", "text": "Wow! Look at this oracle bone character! It's like a tiny picture! So amazing! This is the wisdom of our ancestors from three thousand years ago!"},
    "biden_en": {"path": "/content/drive/MyDrive/Voice data/Joe Biden english.wav", "text": "Wow! Look at this oracle bone character! It's like a tiny picture! So amazing! This is the wisdom of our ancestors from three thousand years ago!"},
    "taylor_en": {"path": "/content/drive/MyDrive/Voice data/Taylor Swift english.wav", "text": "Wow! Look at this oracle bone character! It's like a tiny picture! So amazing! This is the wisdom of our ancestors from three thousand years ago!"},
    "morgan_en": {"path": "/content/drive/MyDrive/Voice data/Morgan Freeman english.wav", "text": "Wow! Look at this oracle bone character! It's like a tiny picture! So amazing! This is the wisdom of our ancestors from three thousand years ago!"},
    "hillary_en": {"path": "/content/drive/MyDrive/Voice data/Hillary Clinton english.wav", "text": "Wow! Look at this oracle bone character! It's like a tiny picture! So amazing! This is the wisdom of our ancestors from three thousand years ago!"},
}

CHARACTER_TRAITS_ZH = {
    "trump": "å·æ™®ï¼šè‡ªå¤§å¤¸å¼ ï¼Œçˆ±è¯´'æœ€æ£’çš„'ã€'æ²¡æœ‰äººæ¯”æˆ‘æ›´æ‡‚'",
    "obama": "æ¬§å·´é©¬ï¼šç†æ€§ä¼˜é›…ï¼Œçˆ±è¯´'è®©æˆ‘å‘Šè¯‰ä½ '",
    "biden": "æ‹œç™»ï¼šäº²åˆ‡è€çˆ·çˆ·ï¼Œçˆ±è¯´'æ¥å§ä¼™è®¡'",
    "taylor": "æ³°å‹’ï¼šæ„Ÿæ€§è¯—æ„ï¼Œç”¨éŸ³ä¹æ¯”å–»",
    "morgan": "æ‘©æ ¹ï¼šç¿æ™ºæ·±æ²‰ï¼Œçºªå½•ç‰‡æ—ç™½æ„Ÿ",
    "hillary": "å¸Œæ‹‰è•Šï¼šä¸“ä¸šçŠ€åˆ©",
    "villager": "æ‘æ°‘ï¼šæ·³æœ´å¤©çœŸ"
}

CHARACTER_TRAITS_EN = {
    "trump": "Trump: arrogant, loves 'the best', 'nobody knows more than me'",
    "obama": "Obama: rational, elegant, 'let me tell you'",
    "biden": "Biden: friendly grandpa, 'come on man'",
    "taylor": "Taylor: emotional, poetic, music metaphors",
    "morgan": "Morgan: wise, deep, documentary narrator",
    "hillary": "Hillary: professional, sharp",
    "villager": "Villager: simple, innocent"
}

CHARACTER_IMAGE_PROMPTS = {
    "trump": "Donald Trump in blue suit with red tie",
    "obama": "Barack Obama in elegant dark suit",
    "biden": "Joe Biden in suit smiling warmly",
    "taylor": "Taylor Swift in glamorous dress",
    "morgan": "Morgan Freeman in wise appearance",
    "hillary": "Hillary Clinton in professional attire",
    "villager": "Chinese village elder in traditional clothes"
}

STORYBOOK_SCENES = [
    "ancient Chinese museum with bronze artifacts, warm lighting",
    "traditional Chinese study room with scrolls and ink brushes, candlelight",
    "ancient Chinese craftsman carving characters, historical workshop",
    "modern classroom with Chinese calligraphy, educational setting",
    "traditional Chinese painting style, mountains and clouds background",
    "celebration of Chinese cultural heritage, red and gold decorations",
]

def is_chinese(lang):
    return 'zh' in lang.lower()

def get_lang_suffix(lang):
    return '_zh' if is_chinese(lang) else '_en'

def get_voice_id(voice_name, lang):
    base = voice_name.replace('_zh', '').replace('_en', '')
    return f"{base}{get_lang_suffix(lang)}"

def get_trait(voice_name, lang):
    base = voice_name.replace('_zh', '').replace('_en', '')
    if is_chinese(lang):
        return CHARACTER_TRAITS_ZH.get(base, "ä¸“ä¸šé£æ ¼")
    return CHARACTER_TRAITS_EN.get(base, "professional style")

def get_char_image(voice_name):
    base = voice_name.replace('_zh', '').replace('_en', '')
    return CHARACTER_IMAGE_PROMPTS.get(base, "a presenter")

def clean_script(text):
    if not text:
        return ""
    preamble_patterns = [
        r'^[Oo]kay[,.]?\s*', r'^[Hh]ere\'?s?\s*(a\s+)?(joke|story|script)[^:ï¼š]*[:ï¼š]\s*',
        r'^[Ss]ure[,!.]?\s*', r'^[Aa]lright[,.]?\s*', r'^[Ll]et me[^:ï¼š]*[:ï¼š]\s*',
        r'^[Hh]ere (is|are)[^:ï¼š]*[:ï¼š]\s*', r'^å¥½çš„[,ï¼Œ]?\s*', r'^ä»¥ä¸‹æ˜¯[^:ï¼š]*[:ï¼š]\s*',
    ]
    for p in preamble_patterns:
        text = re.sub(p, '', text, flags=re.IGNORECASE)
    text = re.sub(r'ï¼ˆ[^ï¼‰]*ï¼‰', '', text)
    text = re.sub(r'\([^)]*\)', '', text)
    text = re.sub(r'ã€[^ã€‘]*ã€‘', '', text)
    text = re.sub(r'\[[^\]]*\]', '', text)
    text = re.sub(r'\*[^*]*\*', '', text)
    text = re.sub(r'ã€Œ([^ã€]*)ã€', r'\1', text)
    patterns = [r'æ·±å¸', r'ç›®å…‰', r'å¾®ç¬‘', r'ç¬‘ç€', r'ç‚¹å¤´', r'æ‘‡å¤´', r'æ²‰æ€',
                r'åœé¡¿', r'æ¸…å—“', r'å¹æ°”', r'çœ¨çœ¼', r'æŒ¥æ‰‹', r'è½¬èº«', r'çœ‹å‘',
                r'sighs', r'smiles', r'nods', r'pauses', r'looks']
    for p in patterns:
        text = re.sub(rf'{p}[^ï¼Œã€‚ï¼ï¼Ÿ,.!?]*[ï¼Œã€‚,.!?]?', '', text, flags=re.IGNORECASE)
    text = re.sub(r'[ï¼Œ,]{2,}', 'ï¼Œ', text)
    text = re.sub(r'^[ï¼Œ,ã€‚\s\n:ï¼š]+', '', text)
    text = text.strip()
    if text.startswith('"') and text.endswith('"'):
        text = text[1:-1]
    return text.strip()

# ===== Ollama (å¢åŠ  timeout) =====

def restart_ollama():
    print("   [Ollama] Restarting...")
    subprocess.run(['pkill', '-9', '-f', 'ollama'], capture_output=True)
    time.sleep(3)
    subprocess.Popen(['ollama', 'serve'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    time.sleep(10)
    print("   [Ollama] Restarted")

def ensure_ollama():
    result = {"ok": False}
    def test():
        try:
            ollama.generate(model=OLLAMA_MODEL, prompt="hi", stream=False)
            result["ok"] = True
        except:
            pass
    t = threading.Thread(target=test)
    t.start()
    t.join(timeout=20)
    if not result["ok"]:
        restart_ollama()
        t = threading.Thread(target=test)
        t.start()
        t.join(timeout=20)
    print("[Ollama] OK" if result["ok"] else "[Ollama] Warning")

def call_ollama(func, **kwargs):
    """å¢åŠ  timeout åˆ° 120 ç§’"""
    for attempt in range(3):
        try:
            result = {"value": None, "error": None}
            def run():
                try:
                    result["value"] = func(**kwargs)
                except Exception as e:
                    result["error"] = e
            t = threading.Thread(target=run)
            t.start()
            t.join(timeout=120)  # å¢åŠ åˆ° 120 ç§’
            if result["value"]:
                return result["value"]
            if result["error"]:
                raise result["error"]
            raise Exception("Ollama timeout")
        except Exception as e:
            print(f"   Ollama attempt {attempt+1} failed: {e}")
            if attempt < 2:
                restart_ollama()
    raise Exception("Ollama failed after 3 attempts")

def extract_json(text):
    """å¾æ–‡æœ¬ä¸­æå– JSONï¼Œè™•ç†å„ç¨®æ ¼å¼"""
    if not text:
        return None

    # ç§»é™¤ markdown code block
    text = re.sub(r'```json\s*', '', text)
    text = re.sub(r'```\s*', '', text)
    text = text.strip()

    # å˜—è©¦ç›´æ¥è§£æ
    try:
        return json.loads(text)
    except:
        pass

    # å˜—è©¦æ‰¾åˆ° JSON å°è±¡
    m = re.search(r'\{[^{}]*\}', text, re.DOTALL)
    if m:
        try:
            return json.loads(m.group())
        except:
            pass

    # å˜—è©¦æ‰¾åˆ°æ›´æ·±å±¤çš„ JSON
    m = re.search(r'\{.*\}', text, re.DOTALL)
    if m:
        try:
            return json.loads(m.group())
        except:
            pass

    return None

def parse_markdown_response(text):
    """å¾ Markdown æ ¼å¼å›æ‡‰ä¸­æå–è³‡è¨Š"""
    result = {"character": "?", "pinyin": "", "meaning": "", "confidence": 0.5}

    # æå–ç¾ä»£ä¸­æ–‡å­—
    patterns = [
        r'\*\*ç¾ä»£ä¸­æ–‡å­—[:\s]*\*\*\s*([^\s\*]+)',
        r'ç¾ä»£ä¸­æ–‡å­—[:\s]*([^\s,ï¼Œ]+)',
        r'character[:\s]*["\']?([^"\'ï¼Œ,\s]+)',
    ]
    for p in patterns:
        m = re.search(p, text, re.IGNORECASE)
        if m:
            char = m.group(1).strip()
            # æ¸…ç†æ‹¬è™Ÿå…§å®¹
            char = re.sub(r'[ï¼ˆ\(].*?[ï¼‰\)]', '', char).strip()
            if char and len(char) <= 4:
                result["character"] = char
                break

    # æå–æ‹¼éŸ³
    patterns = [
        r'\*\*æ‹¼éŸ³[:\s]*\*\*\s*([a-zA-ZÄÃ¡ÇÃ Ä“Ã©Ä›Ã¨Ä«Ã­ÇÃ¬ÅÃ³Ç’Ã²Å«ÃºÇ”Ã¹Ç–Ç˜ÇšÇœ]+\d?)',
        r'æ‹¼éŸ³[:\s]*([a-zA-ZÄÃ¡ÇÃ Ä“Ã©Ä›Ã¨Ä«Ã­ÇÃ¬ÅÃ³Ç’Ã²Å«ÃºÇ”Ã¹Ç–Ç˜ÇšÇœ]+\d?)',
        r'pinyin[:\s]*["\']?([a-zA-ZÄÃ¡ÇÃ Ä“Ã©Ä›Ã¨Ä«Ã­ÇÃ¬ÅÃ³Ç’Ã²Å«ÃºÇ”Ã¹Ç–Ç˜ÇšÇœ]+\d?)',
    ]
    for p in patterns:
        m = re.search(p, text, re.IGNORECASE)
        if m:
            result["pinyin"] = m.group(1).strip()
            break

    # æå–å«ç¾©
    patterns = [
        r'\*\*å«ç¾©[:\s]*\*\*\s*([^*\n]+)',
        r'å«ç¾©[:\s]*([^ï¼Œã€‚\n]{5,50})',
        r'meaning[:\s]*["\']?([^"\']+)',
    ]
    for p in patterns:
        m = re.search(p, text, re.IGNORECASE)
        if m:
            meaning = m.group(1).strip()
            if len(meaning) > 5:
                result["meaning"] = meaning[:100]
                break

    # æå–ä¿¡å¿ƒåº¦
    m = re.search(r'(?:confidence|ä¿¡å¿ƒåº¦)[:\s]*["\']?(0\.\d+|\d+%?)', text, re.IGNORECASE)
    if m:
        conf = m.group(1)
        if '%' in conf:
            result["confidence"] = float(conf.replace('%', '')) / 100
        else:
            result["confidence"] = float(conf)

    return result

# ===== F5-TTS =====

def load_f5():
    global f5_model
    if f5_model is None:
        unload_sdxl()
        unload_svd()
        gc.collect()
        torch.cuda.empty_cache()
        print("   Loading F5-TTS...")
        f5_model = F5TTS()
        print("   F5-TTS loaded")

def unload_f5():
    global f5_model
    if f5_model is not None:
        del f5_model
        f5_model = None
        gc.collect()
        torch.cuda.empty_cache()

def gen_audio(text, voice_id, lang, output_path):
    ref = VOICE_REFERENCES.get(voice_id)
    if not ref or not os.path.exists(ref['path']):
        print(f"   Voice {voice_id} not found, using gTTS")
        tts = gTTS(text=text, lang='zh-TW' if is_chinese(lang) else 'en')
        tts.save(output_path)
        return output_path
    wav, sr, _ = f5_model.infer(
        ref_file=ref['path'], ref_text=ref['text'], gen_text=text,
        nfe_step=F5_NFE_STEP, cfg_strength=F5_CFG_STRENGTH, speed=F5_SPEED
    )
    sf.write(output_path, wav, sr)
    print(f"   Audio saved: {output_path}")
    return output_path

# ===== SDXL =====

def load_sdxl():
    global sdxl_pipe
    if sdxl_pipe is None:
        unload_f5()
        unload_svd()
        gc.collect()
        torch.cuda.empty_cache()
        print("   Loading SDXL...")
        from diffusers import StableDiffusionXLPipeline
        sdxl_pipe = StableDiffusionXLPipeline.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0",
            torch_dtype=torch.float16, variant="fp16", use_safetensors=True
        ).to("cuda")
        sdxl_pipe.enable_model_cpu_offload()
        print("   SDXL loaded")

def unload_sdxl():
    global sdxl_pipe
    if sdxl_pipe is not None:
        del sdxl_pipe
        sdxl_pipe = None
        gc.collect()
        torch.cuda.empty_cache()

def gen_image(prompt, output_path, h=1024, w=1024):
    try:
        load_sdxl()
        print(f"   Generating: {prompt[:50]}...")
        img = sdxl_pipe(
            prompt=prompt,
            negative_prompt="blurry, bad quality, distorted, ugly, text, watermark",
            height=h, width=w, num_inference_steps=30, guidance_scale=7.5
        ).images[0]
        img.save(output_path)
        print(f"   Image saved: {output_path}")
        return True
    except Exception as e:
        print(f"   Image gen error: {e}")
        return False

# ===== SVD =====

def load_svd():
    global svd_pipe
    if svd_pipe is None:
        unload_f5()
        unload_sdxl()
        gc.collect()
        torch.cuda.empty_cache()
        print("   Loading SVD...")
        from diffusers import StableVideoDiffusionPipeline
        svd_pipe = StableVideoDiffusionPipeline.from_pretrained(
            "stabilityai/stable-video-diffusion-img2vid",
            torch_dtype=torch.float16, variant="fp16"
        ).to("cuda")
        svd_pipe.enable_model_cpu_offload()
        print("   SVD loaded")

def unload_svd():
    global svd_pipe
    if svd_pipe is not None:
        del svd_pipe
        svd_pipe = None
        gc.collect()
        torch.cuda.empty_cache()

def gen_video(img_path, output_path):
    try:
        load_svd()
        img = Image.open(img_path).resize((1024, 576), Image.LANCZOS)
        print(f"   Generating video...")
        frames = svd_pipe(img, decode_chunk_size=4, num_frames=SVD_FRAMES, motion_bucket_id=SVD_MOTION).frames[0]
        from diffusers.utils import export_to_video
        export_to_video(frames, output_path, fps=SVD_FPS)
        print(f"   Video saved: {output_path}")
        return True
    except Exception as e:
        print(f"   Video gen error: {e}")
        return False

# ===== è¼”åŠ©å‡½æ•¸ =====

def create_text_frame(char, subtitle="", w=1920, h=1080):
    img = Image.new('RGB', (w, h), (30, 30, 50))
    draw = ImageDraw.Draw(img)
    try:
        font_big = ImageFont.truetype("/usr/share/fonts/truetype/noto/NotoSansCJK-Bold.ttc", 300)
        font_small = ImageFont.truetype("/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc", 48)
    except:
        font_big = ImageFont.load_default()
        font_small = ImageFont.load_default()
    bbox = draw.textbbox((0, 0), char, font=font_big)
    tw, th = bbox[2] - bbox[0], bbox[3] - bbox[1]
    draw.text(((w - tw) / 2, (h - th) / 2 - 80), char, fill=(255, 215, 0), font=font_big)
    if subtitle:
        bbox = draw.textbbox((0, 0), subtitle, font=font_small)
        tw = bbox[2] - bbox[0]
        draw.text(((w - tw) / 2, h - 150), subtitle, fill=(200, 200, 200), font=font_small)
    return img

def create_static_video(img_path, audio_path, output_path):
    from moviepy.editor import ImageClip, AudioFileClip
    audio = AudioFileClip(audio_path)
    clip = ImageClip(img_path).set_duration(audio.duration)
    clip = clip.set_audio(audio)
    clip.fps = 30
    clip.write_videofile(output_path, fps=30, codec="libx264", audio_codec="aac", logger=None)
    audio.close()
    clip.close()
    print(f"   Static video saved: {output_path}")

def merge_video_audio(video_path, audio_path, output_path):
    from moviepy.editor import VideoFileClip, AudioFileClip
    video = VideoFileClip(video_path)
    audio = AudioFileClip(audio_path)
    if video.duration < audio.duration:
        video = video.loop(duration=audio.duration)
    else:
        video = video.subclip(0, audio.duration)
    final = video.set_audio(audio)
    final.write_videofile(output_path, fps=30, codec="libx264", audio_codec="aac", logger=None)
    video.close()
    audio.close()
    final.close()

# ===== SSE é€²åº¦å›å ± =====

def sse_progress(progress, message_zh, message_en=""):
    """SSE æ ¼å¼é€²åº¦äº‹ä»¶"""
    data = {
        "progress": progress,
        "message_zh": message_zh,
        "message_en": message_en or message_zh
    }
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

def sse_complete(video_b64, segments, scripts):
    """SSE æ ¼å¼å®Œæˆäº‹ä»¶"""
    data = {
        "progress": 100,
        "message_zh": "âœ… å®Œæˆï¼",
        "message_en": "âœ… Complete!",
        "video_base64": video_b64,
        "segments": segments,
        "scripts": scripts
    }
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

def sse_error(error_msg):
    """SSE æ ¼å¼éŒ¯èª¤äº‹ä»¶"""
    data = {"error": error_msg}
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

# ===== API Endpoints =====

@app.route('/')
def home():
    return jsonify({"status": "running", "version": "v11-fixed2"})

@app.route('/health')
def health():
    return jsonify({"status": "healthy"})

@app.route('/recognize', methods=['POST'])
@app.route('/recognize/', methods=['POST'])
def recognize():
    print("\n[RECOGNIZE]")
    try:
        img = None
        if 'file' in request.files:
            file = request.files['file']
            if file:
                img_bytes = file.read()
                img = Image.open(io.BytesIO(img_bytes))
                print(f"   Got image: {file.filename}")
        if img is None:
            return jsonify({"error": "No image"}), 400
        if img.mode != 'RGB':
            img = img.convert('RGB')
        buffer = io.BytesIO()
        img.save(buffer, format='PNG')
        buffer.seek(0)
        img_base64 = base64.b64encode(buffer.read()).decode('utf-8')
        print(f"   Calling {VISION_MODEL}...")

        # æ›´å¼·èª¿ JSON æ ¼å¼çš„ prompt
        prompt = """åˆ†æé€™å¼µç”²éª¨æ–‡åœ–ç‰‡ã€‚

è«‹åš´æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼å›ç­”ï¼Œä¸è¦ä½¿ç”¨markdownï¼Œä¸è¦åŠ ä»»ä½•èªªæ˜ï¼š
{"character": "å°æ‡‰çš„ç¾ä»£ä¸­æ–‡å­—", "pinyin": "æ‹¼éŸ³", "meaning": "å«ç¾©èªªæ˜", "confidence": 0.9}

åªè¼¸å‡ºJSONï¼Œä¸è¦å…¶ä»–å…§å®¹ã€‚"""

        response = ollama.chat(
            model=VISION_MODEL,
            messages=[{'role': 'user', 'content': prompt, 'images': [img_base64]}]
        )
        result_text = response['message']['content']
        print(f"   Response: {result_text[:200]}...")

        # å…ˆå˜—è©¦ JSON è§£æ
        result = extract_json(result_text)
        if result and result.get('character') and result.get('character') != '?':
            print(f"   Parsed JSON: {result}")
            return jsonify(result)

        # å¦‚æœ JSON è§£æå¤±æ•—ï¼Œå˜—è©¦å¾ Markdown æ ¼å¼æå–
        print("   JSON parse failed, trying Markdown extraction...")
        result = parse_markdown_response(result_text)
        print(f"   Extracted from Markdown: {result}")

        # å¦‚æœé‚„æ˜¯æ²’æå–åˆ°ï¼Œè¿”å›åŸå§‹å›æ‡‰
        if result.get('character') == '?' and len(result_text) > 10:
            result['meaning'] = result_text[:200]

        return jsonify(result)
    except Exception as e:
        print(f"   Error: {e}")
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route('/joke', methods=['POST'])
@app.route('/joke/', methods=['POST'])
def joke():
    print("\n[JOKE]")
    try:
        char = request.form.get('character', 'å­—')
        meaning = request.form.get('meaning', '')
        lang = request.form.get('lang', 'zh-TW')
        voice = request.form.get('voice_style', 'trump')
        vid = get_voice_id(voice, lang)
        trait = get_trait(voice, lang)
        load_f5()
        if is_chinese(lang):
            p = f'ç”¨{trait}é£æ ¼ä¸ºã€Œ{char}ã€è®²ç¬‘è¯ï¼Œ50-80å­—ç¹ä½“ä¸­æ–‡ï¼Œç¦æ­¢å‰è¨€ï¼Œç›´æ¥ç»™å°è¯ï¼š'
        else:
            p = f'Tell a joke about "{char}" in {trait} style, 50-80 words, NO preamble:'
        r = call_ollama(ollama.generate, model=OLLAMA_MODEL, prompt=p, stream=False)
        text = clean_script(r['response'].strip())
        audio_path = gen_audio(text, vid, lang, "/tmp/joke.wav")
        with open(audio_path, "rb") as f:
            audio_b64 = base64.b64encode(f.read()).decode()
        return jsonify({"joke_text": text, "audio_base64": audio_b64})
    except Exception as e:
        print(f"   Error: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/generate_video', methods=['POST'])
@app.route('/generate_video/', methods=['POST'])
def generate_video():
    print("\n[GENERATE_VIDEO]")
    try:
        char = request.form.get('character', 'å­—')
        meaning = request.form.get('meaning', '')
        voice = request.form.get('voice', 'trump')
        lang = request.form.get('lang', 'zh-TW')
        style_prompt = request.form.get('style_prompt', '')  # ç”¨æˆ¶é¸æ“‡çš„é¢¨æ ¼

        vid = get_voice_id(voice, lang)
        trait = get_trait(voice, lang)
        char_img = get_char_image(voice)

        # å¥—ç”¨ç”¨æˆ¶é¸æ“‡çš„é¢¨æ ¼
        if style_prompt:
            print(f"   Style: {style_prompt}")
            img_prompt = f"a beautiful {meaning} in the center, {char_img} standing next to it, {style_prompt}, detailed, 4k"
        else:
            img_prompt = f"a beautiful {meaning} in the center, {char_img} standing next to it and pointing, museum setting, professional lighting, detailed illustration, 4k"

        print(f"   Image prompt: {img_prompt[:100]}...")

        if is_chinese(lang):
            p = f'ç”¨{trait}é£æ ¼ä¸ºã€Œ{char}ã€ï¼ˆ{meaning}ï¼‰åšè§£è¯´ï¼Œ30-50å­—ç¹ä½“ä¸­æ–‡ï¼Œç¦æ­¢å‰è¨€ï¼š'
        else:
            p = f'Explain "{char}" ({meaning}) in {trait}, 30-50 words, NO preamble:'
        r = call_ollama(ollama.generate, model=OLLAMA_MODEL, prompt=p, stream=False)
        script = clean_script(r['response'].strip()) or f"é€™æ˜¯{char}"
        load_f5()
        audio_path = gen_audio(script, vid, lang, "/tmp/video_audio.wav")
        if not gen_image(img_prompt, "/tmp/video_img.png", h=768, w=1024):
            create_text_frame(char, meaning, w=1024, h=768).save("/tmp/video_img.png")
        img = Image.open("/tmp/video_img.png").resize((1920, 1080), Image.LANCZOS)
        img.save("/tmp/video_img.png")
        create_static_video("/tmp/video_img.png", audio_path, "/tmp/video_out.mp4")
        with open("/tmp/video_out.mp4", "rb") as f:
            b64 = base64.b64encode(f.read()).decode()
        return jsonify({"video_base64": b64, "script": script})
    except Exception as e:
        print(f"   Error: {e}")
        return jsonify({"error": str(e)}), 500

# ==========================================
# ç¹ªæœ¬èªªæ›¸äºº (Streaming)
# ==========================================

@app.route('/storyteller', methods=['POST'])
@app.route('/storyteller/', methods=['POST'])
def storyteller():
    print("\n" + "="*60)
    print("[STORYTELLER - STORYBOOK MODE]")
    print("="*60)

    char = request.form.get('character', 'å­—')
    meaning = request.form.get('meaning', char)
    va = request.form.get('voice_a', 'trump')
    vb = request.form.get('voice_b', 'obama')
    mode = request.form.get('mode', 'single')
    lang = request.form.get('lang', 'zh-TW')
    nsegs = int(request.form.get('segments', 4))
    nsegs = min(max(nsegs, 2), 6)

    def generate():
        try:
            na = va.replace('_zh', '').replace('_en', '')
            nb = vb.replace('_zh', '').replace('_en', '')
            vid_a = get_voice_id(na, lang)
            vid_b = get_voice_id(nb, lang)
            trait_a = get_trait(na, lang)
            trait_b = get_trait(nb, lang)
            cimg_a = get_char_image(na)
            cimg_b = get_char_image(nb)

            print(f"   Char: {char}, Segments: {nsegs}, Mode: {mode}")

            yield sse_progress(5, "æ­£åœ¨ç”Ÿæˆè…³æœ¬...", "Generating script...")

            ensure_ollama()

            # ç”Ÿæˆè…³æœ¬
            if is_chinese(lang):
                if mode == 'dialogue':
                    p = f'''ä½ æ˜¯ç”²éª¨æ–‡æ•™è‚²å°ˆå®¶ã€‚è«‹ç‚ºç”²éª¨æ–‡å­—ã€Œ{char}ã€ï¼ˆæ„æ€ï¼š{meaning}ï¼‰å‰µä½œ{nsegs}æ®µæ•™è‚²å°è©±ã€‚

å°è©±å¿…é ˆåŒ…å«ï¼š
1. é€™å€‹å­—çš„ç”²éª¨æ–‡å­—å½¢åƒä»€éº¼ï¼ˆè±¡å½¢è§£é‡‹ï¼‰
2. å¤äººç‚ºä»€éº¼é€™æ¨£è¨­è¨ˆé€™å€‹å­—
3. é€™å€‹å­—åœ¨å¤ä»£çš„ç”¨é€”æˆ–æ•…äº‹
4. é€™å€‹å­—æ¼”è®Šåˆ°ç¾ä»£çš„éç¨‹

Aï¼ˆ{na}ï¼‰ï¼š{trait_a}
Bï¼ˆ{nb}ï¼‰ï¼š{trait_b}

åš´æ ¼äº¤æ›¿A->B->Aï¼Œæ¯æ®µ30-50å­—ç¹é«”ä¸­æ–‡ã€‚
ç¦æ­¢å‰è¨€ã€ç¦æ­¢å‹•ä½œæè¿°ã€ç¦æ­¢æ‹¬è™Ÿã€‚
JSONï¼š{{"segments":[{{"speaker":"A","script":"å°è©"}},{{"speaker":"B","script":"å°è©"}}...]}}'''
                else:
                    p = f'''ä½ æ˜¯ç”²éª¨æ–‡æ•™è‚²å°ˆå®¶ã€‚è«‹ç”¨{trait_a}é¢¨æ ¼ç‚ºç”²éª¨æ–‡å­—ã€Œ{char}ã€ï¼ˆæ„æ€ï¼š{meaning}ï¼‰å‰µä½œ{nsegs}æ®µæ•™è‚²æ•…äº‹ã€‚

æ•…äº‹å¿…é ˆåŒ…å«ï¼š
1. é€™å€‹å­—çš„ç”²éª¨æ–‡å­—å½¢åƒä»€éº¼ï¼ˆè±¡å½¢è§£é‡‹ï¼‰
2. å¤äººç‚ºä»€éº¼é€™æ¨£è¨­è¨ˆé€™å€‹å­—
3. é€™å€‹å­—åœ¨å¤ä»£çš„ç”¨é€”æˆ–æ–‡åŒ–èƒŒæ™¯

æ¯æ®µ30-50å­—ç¹é«”ä¸­æ–‡ã€‚ç¦æ­¢å‰è¨€ã€ç¦æ­¢æ‹¬è™Ÿã€‚
JSONï¼š{{"segments":[{{"script":"å°è©"}}...]}}'''
            else:
                if mode == 'dialogue':
                    p = f'''You are an ancient Chinese script expert. Create {nsegs} educational dialogue segments about the character "{char}" (meaning: {meaning}).

The dialogue MUST include:
1. What the oracle bone form looks like (pictographic explanation)
2. Why ancient Chinese designed it this way
3. Historical usage or stories about this character

A ({na}): {trait_a}
B ({nb}): {trait_b}

Alternate A->B->A, 30-50 words each, NO preamble.
JSON: {{"segments":[{{"speaker":"A","script":"text"}}...]}}'''
                else:
                    p = f'''You are an ancient Chinese script expert. Create {nsegs} educational story segments about "{char}" (meaning: {meaning}) in {trait_a} style.

The story MUST explain the pictographic origin and cultural significance.
30-50 words each. NO preamble.
JSON: {{"segments":[{{"script":"text"}}...]}}'''

            r = call_ollama(ollama.generate, model=OLLAMA_MODEL, prompt=p, stream=False, format="json")
            d = extract_json(r['response'])

            # Fallback - ä½¿ç”¨èˆ‡å¤æ–‡å­—ç›¸é—œçš„é è¨­å…§å®¹
            if not d or 'segments' not in d or len(d.get('segments', [])) < nsegs:
                print(f"   Fallback: Got {len(d.get('segments', [])) if d else 0} segments, need {nsegs}")
                if is_chinese(lang):
                    d = {"segments": [
                        {"speaker": "A", "script": f"ä½ çœ‹é€™å€‹ã€Œ{char}ã€å­—ï¼åœ¨ç”²éª¨æ–‡è£¡ï¼Œå®ƒçš„å½¢ç‹€å°±åƒçœŸæ­£çš„{meaning}ä¸€æ¨£ï¼Œå¤äººçœŸæ˜¯å¤ªè°æ˜äº†ï¼"},
                        {"speaker": "B", "script": f"æ²’éŒ¯ï¼Œã€Œ{char}ã€æ˜¯å…¸å‹çš„è±¡å½¢å­—ã€‚ä¸‰åƒå¹´å‰çš„äººè§€å¯Ÿ{meaning}çš„æ¨£å­ï¼Œç”¨ç°¡å–®çš„ç·šæ¢æŠŠå®ƒåˆ»åœ¨é¾œç”²ä¸Šã€‚"},
                        {"speaker": "A", "script": f"é€™å€‹å­—è¨­è¨ˆå¾—éå¸¸å¥½ï¼Œç°¡ç›´æ˜¯è—è¡“å“ï¼å¤äººç”¨ã€Œ{char}ã€ä¾†è¨˜éŒ„ç”Ÿæ´»ä¸­çš„{meaning}ï¼Œå¤ªæœ‰æ™ºæ…§äº†ï¼"},
                        {"speaker": "B", "script": f"å¾ç”²éª¨æ–‡åˆ°ç¾ä»£æ¼¢å­—ï¼Œã€Œ{char}ã€ç¶“æ­·äº†ä¸‰åƒå¹´çš„æ¼”è®Šï¼Œä½†æ ¸å¿ƒå½¢è±¡å§‹çµ‚ä¿ç•™è‘—ï¼Œé€™å°±æ˜¯æ¼¢å­—çš„é­…åŠ›ã€‚"},
                        {"speaker": "A", "script": f"æˆ‘è¦è®“å…¨ä¸–ç•Œéƒ½çŸ¥é“ã€Œ{char}ã€é€™å€‹å­—ï¼é€™æ˜¯ä¸­è¯æ–‡åŒ–æœ€æ£’çš„éƒ¨åˆ†ï¼"},
                        {"speaker": "B", "script": f"è®“æˆ‘å€‘ä¸€èµ·å‚³æ‰¿é€™ä»½çè²´çš„æ–‡åŒ–éºç”¢ï¼Œè®“æ›´å¤šäººèªè­˜ç”²éª¨æ–‡çš„æ™ºæ…§ã€‚"},
                    ][:nsegs]}
                else:
                    d = {"segments": [
                        {"speaker": "A", "script": f"Look at this '{char}'! In ancient Chinese script, it looks just like a real {meaning}. Ancient Chinese were brilliant!"},
                        {"speaker": "B", "script": f"Indeed, '{char}' is a pictographic character. 3000 years ago, people observed {meaning} and drew its image with simple strokes."},
                        {"speaker": "A", "script": f"This character design is fantastic! The ancients recorded {meaning} in their daily life with such wisdom!"},
                        {"speaker": "B", "script": f"From ancient script to modern Chinese, '{char}' evolved over 3000 years while preserving its core image."},
                    ][:nsegs]}

            segs = d['segments'][:nsegs]
            for i, s in enumerate(segs):
                s['script'] = clean_script(s.get('script', f'Segment {i+1}'))
                if mode == 'dialogue':
                    s['speaker'] = 'A' if i % 2 == 0 else 'B'
                else:
                    s['speaker'] = 'narrator'

            yield sse_progress(10, f"è…³æœ¬å®Œæˆ ({len(segs)} æ®µ)", f"Script ready ({len(segs)} segments)")

            # ç”Ÿæˆæ¯ä¸€é 
            clips = []
            total = len(segs)

            for i, seg in enumerate(segs):
                page = i + 1
                base = 10 + (i * 80 // total)
                script = seg['script']
                speaker = seg.get('speaker', 'A')

                if mode == 'dialogue':
                    vid = vid_a if speaker == 'A' else vid_b
                    cimg = cimg_a if speaker == 'A' else cimg_b
                else:
                    vid = vid_a
                    cimg = cimg_a

                yield sse_progress(base + 5, f"ç¬¬{page}/{total}é ï¼šèªéŸ³...", f"Page {page}/{total}: Audio...")
                load_f5()
                audio_path = f"/tmp/sb_seg{i}_audio.wav"
                gen_audio(script, vid, lang, audio_path)

                yield sse_progress(base + 15, f"ç¬¬{page}/{total}é ï¼šæ’åœ–...", f"Page {page}/{total}: Image...")
                scene = STORYBOOK_SCENES[i % len(STORYBOOK_SCENES)]

                # åœ–ç‰‡ promptï¼šæ˜ç¢ºè¦æ±‚å‹•ç‰©/äº‹ç‰© + è§’è‰²
                # ä¾‹å¦‚ã€Œç¾Šã€â†’ "a cute sheep" + "Trump pointing at it"
                img_prompt = f"a beautiful {meaning} in the center, {cimg} standing next to it and pointing, {scene}, detailed illustration, children's book style, vibrant colors, 4k"
                print(f"   Image prompt: {img_prompt[:100]}...")

                img_path = f"/tmp/sb_seg{i}_img.png"
                if not gen_image(img_prompt, img_path, h=1080, w=1920):
                    create_text_frame(char, script[:30]).save(img_path)

                yield sse_progress(base + 20, f"ç¬¬{page}/{total}é ï¼šåˆæˆ...", f"Page {page}/{total}: Composing...")
                seg_video = f"/tmp/sb_seg{i}_video.mp4"
                create_static_video(img_path, audio_path, seg_video)
                clips.append(seg_video)

            yield sse_progress(92, "åˆä½µç¹ªæœ¬...", "Assembling...")

            from moviepy.editor import VideoFileClip, concatenate_videoclips
            video_clips = [VideoFileClip(c) for c in clips]
            final = concatenate_videoclips(video_clips, method="compose")
            final.write_videofile("/tmp/sb_final.mp4", fps=30, codec="libx264", audio_codec="aac", logger=None)
            for c in video_clips:
                c.close()

            with open("/tmp/sb_final.mp4", "rb") as f:
                b64 = base64.b64encode(f.read()).decode()

            scripts_out = [{"speaker": s.get('speaker', 'narrator'), "script": s['script']} for s in segs]

            yield sse_complete(b64, len(segs), scripts_out)
            print("   STORYBOOK COMPLETE!")

        except Exception as e:
            print(f"   ERROR: {e}")
            traceback.print_exc()
            yield sse_error(str(e))

    return Response(
        stream_with_context(generate()),
        mimetype='text/event-stream',
        headers={'Cache-Control': 'no-cache', 'X-Accel-Buffering': 'no'}
    )

# ==========================================
# å‹•æ…‹èªªæ›¸äºº (SVD)
# ==========================================

@app.route('/storyteller_animated', methods=['POST'])
@app.route('/storyteller_animated/', methods=['POST'])
def storyteller_animated():
    print("\n" + "="*60)
    print("[STORYTELLER-ANIMATED with SVD]")
    print("="*60)

    char = request.form.get('character', 'å­—')
    meaning = request.form.get('meaning', char)
    va = request.form.get('voice_a', 'trump')
    vb = request.form.get('voice_b', 'obama')
    mode = request.form.get('mode', 'single')
    lang = request.form.get('lang', 'zh-TW')
    nsegs = int(request.form.get('segments', 3))
    nsegs = min(max(nsegs, 2), 4)

    def generate():
        try:
            na = va.replace('_zh', '').replace('_en', '')
            nb = vb.replace('_zh', '').replace('_en', '')
            vid_a = get_voice_id(na, lang)
            vid_b = get_voice_id(nb, lang)
            trait_a = get_trait(na, lang)
            trait_b = get_trait(nb, lang)
            cimg_a = get_char_image(na)
            cimg_b = get_char_image(nb)

            print(f"   Char: {char}, Segments: {nsegs}, Mode: {mode}")

            yield sse_progress(3, "æ­£åœ¨ç”Ÿæˆè…³æœ¬...", "Generating script...")

            ensure_ollama()

            # ç”Ÿæˆè…³æœ¬
            if is_chinese(lang):
                if mode == 'dialogue':
                    p = f'''ä½ æ˜¯ç”²éª¨æ–‡æ•™è‚²å°ˆå®¶ã€‚è«‹ç‚ºç”²éª¨æ–‡å­—ã€Œ{char}ã€ï¼ˆæ„æ€ï¼š{meaning}ï¼‰å‰µä½œ{nsegs}æ®µæ•™è‚²å°è©±ã€‚

å°è©±å¿…é ˆåŒ…å«ï¼š
1. é€™å€‹å­—çš„ç”²éª¨æ–‡å­—å½¢åƒä»€éº¼ï¼ˆè±¡å½¢è§£é‡‹ï¼‰
2. å¤äººç‚ºä»€éº¼é€™æ¨£è¨­è¨ˆé€™å€‹å­—
3. é€™å€‹å­—åœ¨å¤ä»£çš„ç”¨é€”æˆ–æ•…äº‹
4. é€™å€‹å­—æ¼”è®Šåˆ°ç¾ä»£çš„éç¨‹

Aï¼ˆ{na}ï¼‰ï¼š{trait_a}
Bï¼ˆ{nb}ï¼‰ï¼š{trait_b}

åš´æ ¼äº¤æ›¿A->B->Aï¼Œæ¯æ®µ30-50å­—ç¹é«”ä¸­æ–‡ã€‚
ç¦æ­¢å‰è¨€ã€ç¦æ­¢å‹•ä½œæè¿°ã€ç¦æ­¢æ‹¬è™Ÿã€‚
JSONï¼š{{"segments":[{{"speaker":"A","script":"å°è©"}},{{"speaker":"B","script":"å°è©"}}...]}}'''
                else:
                    p = f'''ä½ æ˜¯ç”²éª¨æ–‡æ•™è‚²å°ˆå®¶ã€‚è«‹ç”¨{trait_a}é¢¨æ ¼ç‚ºç”²éª¨æ–‡å­—ã€Œ{char}ã€ï¼ˆæ„æ€ï¼š{meaning}ï¼‰å‰µä½œ{nsegs}æ®µæ•™è‚²æ•…äº‹ã€‚

æ•…äº‹å¿…é ˆåŒ…å«ï¼š
1. é€™å€‹å­—çš„ç”²éª¨æ–‡å­—å½¢åƒä»€éº¼ï¼ˆè±¡å½¢è§£é‡‹ï¼‰
2. å¤äººç‚ºä»€éº¼é€™æ¨£è¨­è¨ˆé€™å€‹å­—
3. é€™å€‹å­—åœ¨å¤ä»£çš„ç”¨é€”æˆ–æ–‡åŒ–èƒŒæ™¯

æ¯æ®µ30-50å­—ç¹é«”ä¸­æ–‡ã€‚ç¦æ­¢å‰è¨€ã€ç¦æ­¢æ‹¬è™Ÿã€‚
JSONï¼š{{"segments":[{{"script":"å°è©"}}...]}}'''
            else:
                if mode == 'dialogue':
                    p = f'''You are an ancient Chinese script expert. Create {nsegs} educational dialogue segments about the character "{char}" (meaning: {meaning}).

The dialogue MUST include:
1. What the oracle bone form looks like (pictographic explanation)
2. Why ancient Chinese designed it this way
3. Historical usage or stories about this character

A ({na}): {trait_a}
B ({nb}): {trait_b}

Alternate A->B->A, 30-50 words each, NO preamble.
JSON: {{"segments":[{{"speaker":"A","script":"text"}}...]}}'''
                else:
                    p = f'''You are an ancient Chinese script expert. Create {nsegs} educational story segments about "{char}" (meaning: {meaning}) in {trait_a} style.

The story MUST explain the pictographic origin and cultural significance.
30-50 words each. NO preamble.
JSON: {{"segments":[{{"script":"text"}}...]}}'''

            r = call_ollama(ollama.generate, model=OLLAMA_MODEL, prompt=p, stream=False, format="json")
            d = extract_json(r['response'])

            # Fallback - ä½¿ç”¨èˆ‡å¤æ–‡å­—ç›¸é—œçš„é è¨­å…§å®¹
            if not d or 'segments' not in d or len(d.get('segments', [])) < nsegs:
                print(f"   Fallback: Got {len(d.get('segments', [])) if d else 0} segments, need {nsegs}")
                if is_chinese(lang):
                    d = {"segments": [
                        {"speaker": "A", "script": f"ä½ çœ‹é€™å€‹ã€Œ{char}ã€å­—ï¼åœ¨ç”²éª¨æ–‡è£¡ï¼Œå®ƒçš„å½¢ç‹€å°±åƒçœŸæ­£çš„{meaning}ä¸€æ¨£ï¼Œå¤äººçœŸæ˜¯å¤ªè°æ˜äº†ï¼"},
                        {"speaker": "B", "script": f"æ²’éŒ¯ï¼Œã€Œ{char}ã€æ˜¯å…¸å‹çš„è±¡å½¢å­—ã€‚ä¸‰åƒå¹´å‰çš„äººè§€å¯Ÿ{meaning}çš„æ¨£å­ï¼Œç”¨ç°¡å–®çš„ç·šæ¢æŠŠå®ƒåˆ»åœ¨é¾œç”²ä¸Šã€‚"},
                        {"speaker": "A", "script": f"é€™å€‹å­—è¨­è¨ˆå¾—éå¸¸å¥½ï¼Œç°¡ç›´æ˜¯è—è¡“å“ï¼å¤äººç”¨ã€Œ{char}ã€ä¾†è¨˜éŒ„ç”Ÿæ´»ä¸­çš„{meaning}ï¼Œå¤ªæœ‰æ™ºæ…§äº†ï¼"},
                        {"speaker": "B", "script": f"å¾ç”²éª¨æ–‡åˆ°ç¾ä»£æ¼¢å­—ï¼Œã€Œ{char}ã€ç¶“æ­·äº†ä¸‰åƒå¹´çš„æ¼”è®Šï¼Œä½†æ ¸å¿ƒå½¢è±¡å§‹çµ‚ä¿ç•™è‘—ï¼Œé€™å°±æ˜¯æ¼¢å­—çš„é­…åŠ›ã€‚"},
                    ][:nsegs]}
                else:
                    d = {"segments": [
                        {"speaker": "A", "script": f"Look at this '{char}'! In ancient Chinese script, it looks just like a real {meaning}. Ancient Chinese were brilliant!"},
                        {"speaker": "B", "script": f"Indeed, '{char}' is a pictographic character. 3000 years ago, people observed {meaning} and drew its image with simple strokes."},
                        {"speaker": "A", "script": f"This character design is fantastic! The ancients recorded {meaning} in their daily life with such wisdom!"},
                        {"speaker": "B", "script": f"From ancient script to modern Chinese, '{char}' evolved over 3000 years while preserving its core image."},
                    ][:nsegs]}

            segs = d['segments'][:nsegs]
            for i, s in enumerate(segs):
                s['script'] = clean_script(s.get('script', f'Segment {i+1}'))
                if mode == 'dialogue':
                    s['speaker'] = 'A' if i % 2 == 0 else 'B'
                else:
                    s['speaker'] = 'narrator'

            yield sse_progress(8, f"è…³æœ¬å®Œæˆ ({len(segs)} æ®µå‹•ç•«)", f"Script ready ({len(segs)} animations)")

            # ç”Ÿæˆæ¯æ®µå‹•ç•«
            clips = []
            total = len(segs)

            for i, seg in enumerate(segs):
                num = i + 1
                base = 8 + (i * 85 // total)
                script = seg['script']
                speaker = seg.get('speaker', 'A')

                if mode == 'dialogue':
                    vid = vid_a if speaker == 'A' else vid_b
                    cimg = cimg_a if speaker == 'A' else cimg_b
                else:
                    vid = vid_a
                    cimg = cimg_a

                yield sse_progress(base + 3, f"å‹•ç•«{num}/{total}ï¼šèªéŸ³...", f"Anim {num}/{total}: Audio...")
                load_f5()
                audio_path = f"/tmp/seg{i}_audio.wav"
                gen_audio(script, vid, lang, audio_path)

                yield sse_progress(base + 8, f"å‹•ç•«{num}/{total}ï¼šåœ–ç‰‡...", f"Anim {num}/{total}: Image...")

                # åœ–ç‰‡ promptï¼šæ˜ç¢ºè¦æ±‚å‹•ç‰©/äº‹ç‰© + è§’è‰²
                img_prompt = f"a beautiful {meaning} in the center, {cimg} standing next to it and explaining, educational illustration, children's book style, vibrant colors, 4k"
                print(f"   Image prompt: {img_prompt[:100]}...")

                img_path = f"/tmp/seg{i}_img.png"
                if not gen_image(img_prompt, img_path):
                    create_text_frame(char, script[:20]).save(img_path)

                yield sse_progress(base + 15, f"å‹•ç•«{num}/{total}ï¼šSVD â³", f"Anim {num}/{total}: SVD â³")
                vid_path = f"/tmp/seg{i}_vid.mp4"
                if not gen_video(img_path, vid_path):
                    from moviepy.editor import ImageClip
                    ic = ImageClip(img_path).set_duration(4)
                    ic.fps = 30
                    ic.write_videofile(vid_path, fps=30, codec="libx264", logger=None)

                yield sse_progress(base + 22, f"å‹•ç•«{num}/{total}ï¼šåˆä½µ...", f"Anim {num}/{total}: Merge...")
                final_path = f"/tmp/seg{i}_final.mp4"
                merge_video_audio(vid_path, audio_path, final_path)
                clips.append(final_path)

            yield sse_progress(95, "åˆä½µæ‰€æœ‰å‹•ç•«...", "Concatenating...")

            from moviepy.editor import VideoFileClip, concatenate_videoclips
            video_clips = [VideoFileClip(c) for c in clips]
            final = concatenate_videoclips(video_clips, method="compose")
            final.write_videofile("/tmp/final.mp4", fps=30, codec="libx264", audio_codec="aac", logger=None)
            for c in video_clips:
                c.close()

            with open("/tmp/final.mp4", "rb") as f:
                b64 = base64.b64encode(f.read()).decode()

            scripts_out = [{"speaker": s.get('speaker', 'narrator'), "script": s['script']} for s in segs]

            yield sse_complete(b64, len(segs), scripts_out)
            print("   ANIMATED COMPLETE!")

        except Exception as e:
            print(f"   ERROR: {e}")
            traceback.print_exc()
            yield sse_error(str(e))

    return Response(
        stream_with_context(generate()),
        mimetype='text/event-stream',
        headers={'Cache-Control': 'no-cache', 'X-Accel-Buffering': 'no'}
    )

# ===== å•Ÿå‹• =====

if __name__ == '__main__':
    print("\n" + "="*60)
    print("   ORACLE BONE DICTIONARY v11-fixed2")
    print("="*60)

    ensure_ollama()

    print("\n[Voices]")
    for n, r in VOICE_REFERENCES.items():
        ok = "OK" if os.path.exists(r['path']) else "X"
        print(f"   {n}: {ok}")

    from pyngrok import ngrok
    try:
        from google.colab import userdata
        ngrok_token = userdata.get('NGROK_AUTHTOKEN')
        if ngrok_token:
            ngrok.set_auth_token(ngrok_token)
            print("\n[ngrok] Token loaded")
    except:
        pass

    url = ngrok.connect(5000)
    print(f"\n{'='*60}")
    print(f"   URL: {url}")
    print(f"{'='*60}\n")

    app.run(host='0.0.0.0', port=5000, debug=False)